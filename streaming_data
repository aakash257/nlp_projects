import urllib.request
import re
from collections import Counter

def download_and_count_streaming(url):
    try:
        response = urllib.request.urlopen(url)
        word_counter = Counter()
        
        # Reading the response in chunks
        while True:
            chunk = response.read(1024).decode('utf-8')  # Read in 1KB chunks
            if not chunk:
                break
            
            # Process the chunk to extract words and update the counter
            words = re.findall(r'\b\w+\b', chunk.lower())
            word_counter.update(words)

        return word_counter

    except Exception as e:
        print(f"Error processing {url}: {e}")
        return Counter()

# Test function
def main(urls):
    total_word_count = Counter()
    for url in urls:
        total_word_count.update(download_and_count_streaming(url))
    
    for word, count in total_word_count.items():
        print(f'{word}: {count}')

if __name__ == "__main__":
    urls = ["http://example.com/some-large-text-file1.txt", "http://example.com/some-large-text-file2.txt"]
    main(urls)

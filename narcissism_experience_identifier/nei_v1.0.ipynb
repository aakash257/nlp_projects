{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aakash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import text_preprocessing as tp\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify narcissism in text \n",
    "\n",
    "I have met a few narcissists in my life and I think a few aspects of their communication come out so profoundly that it motivated me to identify/quantify dimensions of narcissistic personality that contribute to these aspects.\n",
    "\n",
    "My goal is to identify multiple dimensions or single prominent dimension of narcissism, thereby defining a semantic space for narcissism using these dimension/s. I want to use my text mining skills to create such a semantic space and map a random document of natural language based text in this space to quantify narcissism embedded in an unseen text. Such a tool, that could quantify narcissism in natural language, could help identify interactions that could potentially be unhealthy in long run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the steps that we follow:\n",
    "#1. Load dataset of individual experiences of narcissism\n",
    "#2. Preprocess texts in the dataset by filtering ununsual words, stop words, punctuations, and tokenization\n",
    "#3. Compute tfidf values for words in each of the preprocessed text\n",
    "#4. Apply TruncatedSVD on tfidf values to transform into two dimensional space\n",
    "#5. Project a random document on transformed 2D space\n",
    "#6. Compute cosine similarity between random document and first principal component to quantify degree of association of a document with the narcissism's first principal component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the dataset of individual experiences of narcissism\n",
    "\n",
    "The dataset, used to extract important dimensions of narcissism, is the voice of the individuals that exhibited some form of relationship with a narcissist. There is a higher chance of understanding the dimensions of narcissism through these people's voices as they could tell the real behavior of an individual who exhibits multiple personality characteristics that vary depending on the relationship you are exhibiting with such narcissistic individuals. \n",
    "\n",
    "The first principal component is the representation of an important dimension or dimensions of narcissism that contribute to their behavior. If there are multiple dimensions, it will be hard to capture these dimensions as these personality characteristics based dimensions might co-occur and hence correlate among individuals. So, identifying orthogonal dimensions could be a difficult, if not tedious, task. One important point to note here is that data points lying on the axes of each of the orthogonal dimensions represents that these data points has no association with other dimensions (zero coordinate values). \n",
    "\n",
    "If a text projected onto this dimension has a large projected length, then we can say that the words in the text are associated with or belong to the first prinicipal component extracted from texts describing experience with narcissism. We can say that this text has high degree of narcissism.\n",
    "\n",
    "We will not be able to tag a narcissist from the voice of the individuals who were in some form of relationship with a narcissist because the voice is the experience with a narcissist, so a perception of a narcissist from the experiencer. We will be able to tag the experience of a relationship with a narcissist. Can the perception of a narcissist be used to tag a narcissist? We could tag a narcissist from the perception of the individual who is interacting with the narcissist. But, we are not sure if we will be able to tag a narcissist in general, without getting a lot of data that help identifies subtle cues of narcissism that are only evident to the individuals who have prolong contact with narcissists. So, the application we are trying to build is going to help the individuals who are in some form of relationship with a narcissist. \n",
    "\n",
    "If a text represents thoughts of or about a narcissist, that could create perceptions among perceivers. Perceptions about narccissism in the form of experiences with narcissists have been captured in the first principal component. So if association between text that creates/represents perceptions about a narcissist and the principal component that represents the experience of individuals with narcissists then the text captures perception of/about narcissism. This association can be used as an indicator of a relationship between text and narcissism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_folder_path = os.getcwd()\n",
    "folder_path = 'C:\\\\Users\\\\Aakash\\\\Desktop\\\\my_py_pro\\\\narcissism\\\\data'\n",
    "text_files = []\n",
    "\n",
    "os.chdir(folder_path)\n",
    "for file in glob.glob(\"*.txt\"):\n",
    "    text_files.append(file)\n",
    "os.chdir(initial_folder_path)\n",
    "\n",
    "folder_path = 'C:\\\\Users\\\\Aakash\\\\Desktop\\\\my_py_pro\\\\narcissism\\\\data'\n",
    "os.chdir(folder_path)\n",
    "narc_experience_texts = []\n",
    "for file in text_files:\n",
    "    with open(file, encoding=\"utf8\") as f:\n",
    "        narc_experience_texts.append(' '.join(f.read().splitlines()))\n",
    "os.chdir(initial_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Preprocess texts in the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cleaned paragraphs in the pdf:  11\n"
     ]
    }
   ],
   "source": [
    "preprocess_obj = tp.PreprocessDocs() # default argument values are 'lemmatization' = 'yes', 'stemming' = no, 'stopwords_remove' = 'yes'\n",
    "doc_min_size = 5 # see text_preprocessing.py for description of variables\n",
    "clean_docs = preprocess_obj.docs_preprocess(narc_experience_texts, doc_min_size)\n",
    "\n",
    "print(\"Number of cleaned paragraphs in the pdf: \", len(clean_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Compute tfidf values for words in each of the preprocessed text\n",
    "\n",
    "TFIDF penalizes words that are common across experiences. We would like to have such words not penalized heavily as they represent experiences common across individuals in some form of relationship with narcissists. \n",
    "\n",
    "Using a TFIDF measure that penalizes words common across documents will extract words that are more unique to the individual experiences than experiences common among all individuals. We do not want low tfidf values for the words that are common across individual experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tfidf CountVectorizer provided in scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [' '.join(doc) for doc in clean_docs]\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "corpus_wlist = vectorizer.vocabulary_.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Apply TruncatedSVD on tfidf values to transform into two dimensional space\n",
    "\n",
    "In order to identify dimensions of narcissism, comprising of words that appear in narcissistic experiences, we use Principal Component Analysis.\n",
    "\n",
    "Principal Component Analysis is a dimension reduction technique that identifies direction of maximum variance among sample data. In our case, we find the direction of maximum variance in tfidf values. One good thing about PCA is that it does not assume any prior distribution associated with the sample data. Hence, it is a non parametric method for dimension reduction. In our sample data of tfidf values of words in narcissistic texts, we do not know the distribution. Hence, PCA fits well in our situation. \n",
    "\n",
    "We use TruncatedSVD instead of PCA as we have very sparse data, a very few words are common across narcissistic texts resulting in a lot of zero tfidf values in each text. In truncatedSVD, we do not center data as most of the values are zero, the mean will be close to zero. (I have to check the difference between TruncatedSVD and SVD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>principalcomponent1</th>\n",
       "      <th>principalcomponent2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.529048</td>\n",
       "      <td>-0.035848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.447896</td>\n",
       "      <td>-3.287245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.641530</td>\n",
       "      <td>-1.967149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.513618</td>\n",
       "      <td>-2.008614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.510506</td>\n",
       "      <td>-0.408981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11.021033</td>\n",
       "      <td>-7.116664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.747444</td>\n",
       "      <td>10.862772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.579621</td>\n",
       "      <td>0.472877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.467273</td>\n",
       "      <td>-0.714870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.259381</td>\n",
       "      <td>0.149011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.724853</td>\n",
       "      <td>-0.632366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    principalcomponent1  principalcomponent2\n",
       "0              2.529048            -0.035848\n",
       "1              7.447896            -3.287245\n",
       "2              3.641530            -1.967149\n",
       "3              2.513618            -2.008614\n",
       "4              2.510506            -0.408981\n",
       "5             11.021033            -7.116664\n",
       "6             10.747444            10.862772\n",
       "7              4.579621             0.472877\n",
       "8              2.467273            -0.714870\n",
       "9              1.259381             0.149011\n",
       "10             1.724853            -0.632366"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "pca = TruncatedSVD(n_components=2, random_state=42)\n",
    "principalComponents = pca.fit_transform(X)\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns=['principalcomponent1', 'principalcomponent2'])\n",
    "#principalDf['label'] = corpus_wlist\n",
    "pc1_df = pd.DataFrame(principalDf['principalcomponent1'].sort_values(ascending=False).iloc[:30])\n",
    "pc2_df = pd.DataFrame(principalDf['principalcomponent2'].sort_values(ascending=False).iloc[:30])\n",
    "pc12_df = pd.concat([pc1_df, pc2_df], axis = 1, join = 'outer', ignore_index=False, sort=False)\n",
    "pc12_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab</th>\n",
       "      <th>pc1</th>\n",
       "      <th>pc2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>absolute</td>\n",
       "      <td>0.030928</td>\n",
       "      <td>-0.037718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>absolutely</td>\n",
       "      <td>0.025741</td>\n",
       "      <td>-0.020774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>absorb</td>\n",
       "      <td>0.007045</td>\n",
       "      <td>-0.002168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abuse</td>\n",
       "      <td>0.041339</td>\n",
       "      <td>-0.038274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abuser</td>\n",
       "      <td>0.013215</td>\n",
       "      <td>-0.005913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>wonder</td>\n",
       "      <td>0.010219</td>\n",
       "      <td>-0.010426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>word</td>\n",
       "      <td>0.030161</td>\n",
       "      <td>0.057572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>worth</td>\n",
       "      <td>0.010219</td>\n",
       "      <td>-0.010426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>write</td>\n",
       "      <td>0.030928</td>\n",
       "      <td>-0.037718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>wrong</td>\n",
       "      <td>0.020901</td>\n",
       "      <td>-0.017422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>477 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          vocab       pc1       pc2\n",
       "0      absolute  0.030928 -0.037718\n",
       "1    absolutely  0.025741 -0.020774\n",
       "2        absorb  0.007045 -0.002168\n",
       "3         abuse  0.041339 -0.038274\n",
       "4        abuser  0.013215 -0.005913\n",
       "..          ...       ...       ...\n",
       "472      wonder  0.010219 -0.010426\n",
       "473        word  0.030161  0.057572\n",
       "474       worth  0.010219 -0.010426\n",
       "475       write  0.030928 -0.037718\n",
       "476       wrong  0.020901 -0.017422\n",
       "\n",
       "[477 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pc = pd.DataFrame(vectorizer.vocabulary_.items(), columns=['vocab', 'order'])\n",
    "df_pc.sort_values(by=['order'], inplace=True)\n",
    "df_pc['pc1'] = pca.components_[0]\n",
    "df_pc['pc2'] = pca.components_[1]\n",
    "df_pc = df_pc.reset_index(drop=True)\n",
    "del df_pc['order']\n",
    "df_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "df_pc.plot.scatter(x='pc1',y='pc2')\n",
    "\n",
    "for i in range(df_pc.shape[0]):\n",
    "    plt.text(x=df_pc.pc1[i], y=df_pc.pc2[i], s=df_pc.vocab[i])\n",
    "\n",
    "plt.xlabel(\"pc1\") #x label\n",
    "plt.ylabel(\"pc2\") #y label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Project a random document on transformed 2D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.44789563, -3.2872451 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_document = narc_experience_texts[1]\n",
    "\n",
    "# Preprocess new document\n",
    "preprocessed_new_doc = preprocess_obj.docs_preprocess([new_document], doc_min_size)[0]\n",
    "\n",
    "# Vectorize the new document\n",
    "doc_vec = vectorizer.transform([' '.join(preprocessed_new_doc)])\n",
    "\n",
    "# Project vectorized document onto principal components\n",
    "pca.transform(doc_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Quantify narcissism in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.915"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute projection of a document on first principal component\n",
    "round(pca.transform(doc_vec)[0][0]/np.linalg.norm(pca.transform(doc_vec)), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.824"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_document1 = 'Right from things like playing loud music to closing the doors behind loudly. Not only these objective behaviours. But also the conversations were always turned around to his problems. Whenever it was about my interests, they were made fun of and ridiculed. Soon enough, I decided to have no contact with my flatmate and started having very few conversations, like once a week. When I confronted him about the way he converses, he immediately said this is not me and that he cannot have said any of that. Although, he did accepted points around loud noice and late night sounds and kept them low. But the conversations according to me would have never improved and hence I kept my distance.'\n",
    "# Preprocess new document\n",
    "preprocessed_new_doc = preprocess_obj.docs_preprocess([new_document1], doc_min_size)[0]\n",
    "\n",
    "# Vectorize the new document\n",
    "doc_vec = vectorizer.transform([' '.join(preprocessed_new_doc)])\n",
    "\n",
    "# Project vectorized document onto principal components\n",
    "pca.transform(doc_vec)\n",
    "round(pca.transform(doc_vec)[0][0]/np.linalg.norm(pca.transform(doc_vec)), 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
